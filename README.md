<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python Version">
  <img src="https://img.shields.io/badge/Framework-Flask-green.svg" alt="Flask">
  <img src="https://img.shields.io/badge/Container-Docker-blue.svg" alt="Docker">
  <img src="https://img.shields.io/badge/Deployment-AWS%20|%20Azure-orange.svg" alt="Deployment">
  <img src="https://img.shields.io/badge/License-MIT-lightgrey.svg" alt="License">
</p>

---
<h1 align="center">End-to-End Machine Learning Project</h1>

## ğŸ“˜ Overview
This repository contains a **fully modular, production-ready Machine Learning project** built from scratch and deployed on **AWS** and **Azure** using **Docker containerization**.

The project demonstrates the complete lifecycle of an ML application from **data ingestion and transformation** to **model training**, **Flask API deployment**, and **cloud deployment** following best software engineering practices like **logging**, **custom exception handling**, and **config-driven modularization**.

## ğŸ§± Project Architecture
```
project-root/
â”‚
â”œâ”€â”€ artifacts/ # Stored artifacts and serialized models
â”‚ â”œâ”€â”€ data.csv
â”‚ â”œâ”€â”€ train.csv
â”‚ â”œâ”€â”€ test.csv
â”‚ â””â”€â”€ preprocessor.pkl
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ stud.csv # Raw dataset
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for experiments
â”‚ â”œâ”€â”€ 01.eda.ipynb
â”‚ â””â”€â”€ 02.modeling.ipynb
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ components/ # Data and model pipeline components
â”‚ â”‚ â”œâ”€â”€ data_ingestion.py
â”‚ â”‚ â”œâ”€â”€ data_transformation.py
â”‚ â”‚ â””â”€â”€ model_trainer.py
â”‚ â”‚
â”‚ â”œâ”€â”€ pipeline/ # End-to-end pipeline orchestration
â”‚ â”‚ â””â”€â”€ training_pipeline.py
â”‚ â”‚
â”‚ â”œâ”€â”€ exception.py # Custom exception handler
â”‚ â”œâ”€â”€ logger.py # Logging configuration
â”‚ â”œâ”€â”€ utils.py # Helper functions
â”‚ â””â”€â”€ init.py
â”‚
â”œâ”€â”€ app.py # Flask API for model inference
â”œâ”€â”€ Dockerfile # Docker container configuration
â”œâ”€â”€ setup.py # Project packaging setup
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ§  Features
- ğŸ”„ **End-to-End ML Pipeline:** Data ingestion â†’ transformation â†’ model training â†’ evaluation  
- ğŸ§¹ **Automated Preprocessing:** Handles missing values, encoding, and scaling  
- ğŸ§ª **Exploratory Analysis:** Jupyter notebooks for EDA and feature engineering  
- ğŸªµ **Logging System:** Real-time tracking of all pipeline stages (`logger.py`)  
- âš¡ **Custom Exception Handling:** Graceful error capture and debugging via `CustomException`  
- ğŸ§° **Utility Functions:** Common operations abstracted into `utils.py`  
- ğŸŒ **Flask REST API:** Exposes prediction endpoints for integration  
- ğŸ³ **Docker Containerization:** Consistent, portable environment for deployment  
- â˜ï¸ **Cloud Deployment:** Hosted on **AWS EC2** and **Azure Web App**  
- ğŸ§© **Configurable Setup:** Uses `setup.py` and `requirements.txt` for easy installation  

## ğŸ§© Key Components
| Module | Description |
|--------|-------------|
| **`data_ingestion.py`** | Loads and validates data from source, splits into train/test sets |
| **`data_transformation.py`** | Cleans data, performs feature engineering, and builds preprocessing pipeline |
| **`model_trainer.py`** | Trains, tunes, and evaluates ML models; saves best model to `artifacts/` |
| **`logger.py`** | Centralized logging utility for all pipeline steps |
| **`exception.py`** | Custom error-handling framework with detailed traceback logging |
| **`utils.py`** | Helper utilities (e.g., file handling, model saving/loading) |
| **`app.py`** | Flask application for model inference |
| **`Dockerfile`** | Defines environment for containerized deployment |

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment
```
python -m venv venv
```
```
source venv/bin/activate      # macOS/Linux
````
```
venv\Scripts\activate         # Windows
```

### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Run the ML Pipeline
```bash
python -m src.pipeline.training_pipeline
```

### 5ï¸âƒ£ Start the Flask API
```bash
python app.py
```
Your API will be available at:

ğŸ‘‰ http://127.0.0.1:5000/predict

### ğŸ³ Running with Docker
Build Docker Image
```bash
docker build -t rentify-ml-app .
```

### Run Docker Container
`docker run -p 5000:5000 rentify-ml-app`

Access the API at:
ğŸ‘‰ http://localhost:5000/predict

## â˜ï¸ Deployment
### AWS EC2

- Launch an EC2 instance 

- Install Docker and pull your image from Docker Hub

- Run the container and expose port 5000

### Azure Web App

- Create a Web App for Containers

- Push your Docker image to Azure Container Registry (ACR)

- Deploy the container to your Web App

## ğŸ§¾ Logging & Monitoring

- Logs are automatically generated and stored in the logs/ directory.

- Each major pipeline component logs start, end, and error states.

- Critical exceptions are captured by CustomException and written to both console and log files.

## ğŸ§° Tech Stack

| Category | Tools / Libraries |
|-----------|------------------|
| **Language** | Python 3.10+ |
| **Data Handling** | Pandas, NumPy |
| **ML Framework** | Scikit-learn |
| **API Framework** | Flask |
| **Containerization** | Docker |
| **Deployment** | AWS EC2, Azure Web App |
| **Utilities** | Logging, OS, sys, pickle |


## ğŸ“ˆ Future Enhancements

- Model versioning with MLflow

- Front-end interface (Streamlit or React)

- API authentication and rate limiting

## ğŸ‘¨â€ğŸ’» Author
**Derrick Nyongesa**  
B.Sc. Electrical & Electronics Engineering | Data Scientist  
ğŸ“§ [derricknyongesa0.email@gmail.com](derricknyongesa0.email@gmail.com)  
ğŸŒ [LinkedIn](https://www.linkedin.com/in/derrick-nyongesa/) | [GitHub](https://github.com/DECTEN0/)



